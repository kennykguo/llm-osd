# next task: llm-provider (llama.cpp local)

## objective

add a local llama.cpp-backed provider that produces schema-valid `ActionPlan` json with constrained decoding.

## milestones

### milestone 1: provider skeleton (no model yet)

- add new rust crate `llm-provider`
- cli accepts `--prompt` (or stdin) and prints a placeholder plan
- add tests that the placeholder output parses + validates via `llm-os-common`

### milestone 2: llama.cpp subprocess integration (no grammar yet)

- run `llama-cli` once per request, capture stdout
- implement strict extraction: output must be json-only, no extra text
- post-validate output with `llm-os-common`

### milestone 3: constrained decoding (fixed actionplan schema)

- add `actionplan.gbnf`
- invoke llama.cpp with grammar enabled
- add tests that invalid output is rejected and that valid output is accepted

### milestone 4: workflow integration

- decide whether to:
  - emit `plan_only` only, or
  - allow `--mode execute`
- optionally add `llmsh` integration helpers (separate from daemon execution)

## explicit questions to resolve before milestone 2

- which llama.cpp binary do you want to standardize on (`llama-cli` vs `llama-server`)?
- what model file path should the provider expect by default?
- do you want a hard requirement that the provider always outputs `plan_only` first?


